import os
import json
import asyncio
from dotenv import load_dotenv
from openai import AsyncOpenAI
from tqdm import tqdm

# =============================
# 1. Load env
# =============================
load_dotenv()
API_KEY = os.getenv("ALICE_API_KEY")
EMB_URL = os.getenv("ALICE_EMB_URL")

client = AsyncOpenAI(api_key=API_KEY, base_url=EMB_URL)


# =============================
# 2. Embed batch
# =============================
async def embed_batch(text_list, desc="Embedding"):
    embeddings = []
    batch_size = 256  # ÏõêÌïòÎäî batch sizeÎ°ú Ï°∞Ï†à Í∞ÄÎä•

    for i in tqdm(range(0, len(text_list), batch_size), desc=desc):
        batch = text_list[i:i+batch_size]
        res = await client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        for item in res.data:
            embeddings.append(item.embedding)

    return embeddings


# =============================
# 3. Load dataset
# =============================
DATA_PATH = "dataset/2wikimultihopqa.json"
OUTPUT_PATH = "dataset/2wikimultihopqa_embeddings.jsonl"

with open(DATA_PATH, "r", encoding="utf-8") as f:
    dataset = json.load(f)

print(f"Loaded {len(dataset)} samples.")


# =============================
# 4. Step 1 ‚Äî Í∏ÄÎ°úÎ≤å Î¶¨Ïä§Ìä∏ Íµ¨ÏÑ±
# =============================
query_texts = []
sf_texts = []
sf_map = []         # supporting fact ÏúÑÏπò Ï†ïÎ≥¥
passage_texts = []
passage_map = []     # passage ÏúÑÏπò Ï†ïÎ≥¥

for sample_idx, sample in enumerate(dataset):
    question = sample["question"]
    query_texts.append(question)

    # -------- Supporting Facts ---------
    supporting_facts = sample["supporting_facts"]
    context = sample["context"]

    # supporting fact sentence Ï∂îÏ∂ú
    for title, idx in supporting_facts:
        found = False
        for ctx_title, sentences in context:
            if ctx_title == title:
                if 0 <= idx < len(sentences):
                    sent = sentences[idx]
                    sf_texts.append(sent)
                    sf_map.append({
                        "sample_idx": sample_idx,
                        "sf_local_idx": len(sf_map)  # global index
                    })
                found = True
                break
        if not found:
            continue

    # gold passage title ÏßëÌï©
    gold_titles = {title for title, _ in supporting_facts}

    for ctx_title, sentences in context:
        passage_text = f"{ctx_title}. " + " ".join(sentences)
        passage_texts.append(passage_text)

        passage_map.append({
            "sample_idx": sample_idx,
            "title": ctx_title,
            "is_gold": ctx_title in gold_titles    # üî• gold Ïó¨Î∂Ä Ï†ÄÏû•
        })


# =============================
# 5. Step 2 ‚Äî Í∏ÄÎ°úÎ≤å ÏûÑÎ≤†Îî© 3Ìöå Ìò∏Ï∂ú
# =============================
async def main():
    print("\n=== Embedding Queries ===")
    query_embeddings = await embed_batch(query_texts, desc="Query Embedding")

    print("\n=== Embedding Supporting Facts ===")
    sf_embeddings = await embed_batch(sf_texts, desc="Supporting Fact Embedding")

    print("\n=== Embedding Passages ===")
    passage_embeddings = await embed_batch(passage_texts, desc="Passage Embedding")

    # =============================
    # 6. Step 3 ‚Äî sampleÎ≥ÑÎ°ú Ïû¨Ï°∞Î¶Ω
    # =============================
    output_samples = [{} for _ in range(len(dataset))]

    # Í∏∞Î≥∏ ÌãÄ ÏÉùÏÑ±
    for i, sample in enumerate(dataset):
        output_samples[i] = {
            "query": sample["question"],
            "query_embedding": query_embeddings[i],

            "supporting_facts": [],
            "passages": []
        }

    # Supporting Facts Ïû¨Ï°∞Î¶Ω
    sf_counter = 0
    for sf_idx, (text, emb) in enumerate(zip(sf_texts, sf_embeddings)):
        sample_idx = sf_map[sf_idx]["sample_idx"]
        output_samples[sample_idx]["supporting_facts"].append({
            "sentence": text,
            "embedding": emb
        })

    # Passage Ïû¨Ï°∞Î¶Ω
    for p_idx, (text, emb) in enumerate(zip(passage_texts, passage_embeddings)):
        sample_idx = passage_map[p_idx]["sample_idx"]
        title = passage_map[p_idx]["title"]
        is_gold = passage_map[p_idx]["is_gold"]
        output_samples[sample_idx]["passages"].append({
            "title": title,
            "passage": text,
            "embedding": emb,
            "is_gold": is_gold
        })

    # =============================
    # 7. JSONL Ï†ÄÏû•
    # =============================
    with open(OUTPUT_PATH, "w", encoding="utf-8") as out:
        for record in output_samples:
            out.write(json.dumps(record, ensure_ascii=False) + "\n")

    print("\nEmbedding Completed!")
    print(f"Saved to {OUTPUT_PATH}")


asyncio.run(main())
